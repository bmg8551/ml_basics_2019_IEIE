{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "5.EM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanglee/ml_basics_2019_IEIE/blob/master/5_EM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1neNbkmR7XV3",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "\n",
        "# Machine Learning\n",
        "\n",
        "<br>\n",
        "\n",
        "## Sangkyun Lee <br> Hanyang University ERICA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATXPOM6c7XV-",
        "colab_type": "text"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DulXmYie7XWB",
        "colab_type": "text"
      },
      "source": [
        "### Jensen's Inequality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZRi4MD-7XWD",
        "colab_type": "text"
      },
      "source": [
        "$f$: convex function and $X$: a random variable.\n",
        "Then,\n",
        "$$\n",
        " f(E[X]) \\le E[f(X)]\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-h5ODXJ7XWG",
        "colab_type": "text"
      },
      "source": [
        "If $f$ strictly convex, $f(E[X]) = E[f(X)]$ holds true iff $X = E[X]$ with probability 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjZ3G73T7XWK",
        "colab_type": "text"
      },
      "source": [
        "### EM Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xas7ZiBi7XWM",
        "colab_type": "text"
      },
      "source": [
        "Given: a training set $ \\{x^{(1)},\\dots,x^{(m)} \\} $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ8i09X07XWQ",
        "colab_type": "text"
      },
      "source": [
        "Consider: a joint probability $p(x,z)$, where $z^{(i)}$'s are (unobserved) latent random variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0I3S4wxO7XWT",
        "colab_type": "text"
      },
      "source": [
        "We wish to fit $p(x,z;\\theta)$ to the training data, where the log likelihood is given by:\n",
        "\\begin{align}\n",
        " \\ell(\\theta) &= \\sum_{i=1}^m \\log p(x;\\theta) \\\\\n",
        " &= \\sum_{i=1}^m \\log \\sum_z p(x,z;\\theta)\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-_ZPkfj7XWV",
        "colab_type": "text"
      },
      "source": [
        "The EM algorithm provides an efficient way for MLE where $ z^{(i)} $'s are unobserved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuJdGTM37XWa",
        "colab_type": "text"
      },
      "source": [
        "#### E-Step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSUOQoJy7XWb",
        "colab_type": "text"
      },
      "source": [
        "Let $Q_i$ be some distribution over the $ z^{(i)} $'s: $\\sum_z Q_i(z)=1$, $Q_i(z) \\ge 0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJUgK3AF7XWe",
        "colab_type": "text"
      },
      "source": [
        "\\begin{align}\n",
        "\\ell(\\theta) &= \\sum_i \\log \\sum_{z^{(i)}} p(x^{(i)}, z^{(i)}; \\theta) \\\\\n",
        "&= \\sum_i \\log \\sum_{z^{(i)}} Q_i(z^{(i)}) \\frac{p(x^{(i)}, z^{(i)}; \\theta))}{Q_i(z^{(i)})} \\\\\n",
        "&\\ge \\sum_i \\sum_{z^{(i)}} Q_i(z^{(i)}) \\log \\frac{p(x^{(i)}, z^{(i)}; \\theta))}{Q_i(z^{(i)})}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oA0B_Fb7XWf",
        "colab_type": "text"
      },
      "source": [
        "Which $Q_i$ to choose? We want to make the lower bound as tight as possible:\n",
        "$$\n",
        "\\frac{p(x^{(i)}, z^{(i)}; \\theta))}{Q_i(z^{(i)})} = c\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLOHMdr77XWh",
        "colab_type": "text"
      },
      "source": [
        "In other words, make $ Q_i(z^{(i)}) \\propto p(x^{(i)}, z^{(i)}; \\theta)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZEdnA8F7XWj",
        "colab_type": "text"
      },
      "source": [
        "Since $\\sum_z Q_i(z)=1$, this implies that\n",
        "\\begin{align}\n",
        "Q_i(z^{(i)}) &= \\frac{ p(x^{(i)}, z^{(i)}; \\theta) }{ \\sum_z p(x^{(i)}, z; \\theta) } \\\\\n",
        "&= \\frac{ p(x^{(i)}, z^{(i)}; \\theta) }{ p(x^{(i)}; \\theta) } \\\\\n",
        "&= p(z^{(i)} | x^{(i)}; \\theta)\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzCg-OXI7XWl",
        "colab_type": "text"
      },
      "source": [
        "#### M-Step\n",
        "Maximize the tight lower bound over $\\theta$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dY5xaFZ7XWn",
        "colab_type": "text"
      },
      "source": [
        "#### EM Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n-nvJmb7XWo",
        "colab_type": "text"
      },
      "source": [
        "    Repeat until convergence{\n",
        "    \n",
        "        (E-Step) For each i, set\n",
        "$$\n",
        "          Q_i(z^{(i)}) = p(z^{(i)} | x^{(i)} ; \\theta)\n",
        "$$\n",
        "\n",
        "\n",
        "        (M-Step) Set\n",
        "$$\n",
        "          \\theta := \\arg\\max_\\theta \\;  \\sum_i \\sum_{z^{(i)}} Q_i(z^{(i)}) \\log \\frac{p(x^{(i)}, z^{(i)}; \\theta))}{Q_i(z^{(i)})}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9QqG3SF7XWp",
        "colab_type": "text"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNSFD5PM7XWs",
        "colab_type": "text"
      },
      "source": [
        "### Monotonic convergence of EM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNDBDpVs7XWv",
        "colab_type": "text"
      },
      "source": [
        "Suppose that we've started a iteration of the EM algorithm with $\\theta^{(t)}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4BXaehh7XWx",
        "colab_type": "text"
      },
      "source": [
        "Then, E-Step will choose $Q_i^{(t)}(z^{(i)}) = p(z^{(i)} | x^{(i)} ; \\theta^{(t)})$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqX48LTv7XW0",
        "colab_type": "text"
      },
      "source": [
        "We also know that this choice will make the Jensen's inequality to hold as equality, that is,\n",
        "$$\n",
        "\\ell(\\theta^{(t)}) = \\sum_i \\sum_{z^{(i)}} Q_i^{(t)}(z^{(i)}) \\log \\frac{p(x^{(i)}, z^{(i)}; \\theta^{(t)}))}{Q_i^{(t)}(z^{(i)})}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVMxXwJN7XW2",
        "colab_type": "text"
      },
      "source": [
        "If we call the updated $\\theta$ from the M-Step is $\\theta^{(t+1)}$,\n",
        "\\begin{align}\n",
        "\\ell(\\theta^{(t+1)}) &\\ge \\sum_i \\sum_{z^{(i)}} Q_i^{(t)}(z^{(i)}) \\log \\frac{p(x^{(i)}, z^{(i)}; \\theta^{(t+1)}))}{Q_i^{(t)}(z^{(i)})} \\\\\n",
        "&\\ge \\sum_i \\sum_{z^{(i)}} Q_i^{(t)}(z^{(i)}) \\log \\frac{p(x^{(i)}, z^{(i)}; \\theta^{(t)}))}{Q_i^{(t)}(z^{(i)})} \\\\\n",
        "&= \\ell(\\theta^{(t)})\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd7oso4p7XW3",
        "colab_type": "text"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTkRVErE7XW4",
        "colab_type": "text"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siDOpsVW7XW6",
        "colab_type": "text"
      },
      "source": [
        "#### Mixture of Gaussian "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZPAZOTc7XW7",
        "colab_type": "text"
      },
      "source": [
        "Given: a training set $ \\{x^{(1)},\\dots,x^{(m)} \\} $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASNyTWO57XW9",
        "colab_type": "text"
      },
      "source": [
        "We wish to model the data by a joint distribution $p(x^{(i)},z^{(i)}) = p(x^{(i)}|z^{(i)}) p(z^{(i)})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j13grW2S7XW_",
        "colab_type": "text"
      },
      "source": [
        "\\begin{align}\n",
        "\\begin{cases}\n",
        " z^{(i)} & \\sim \\text{Multinomial}(\\phi), \\;\\; \\phi_j \\ge 0, \\sum_{j=1}^k \\phi_j =1\\\\\n",
        " x^{(i)} | z^{(i)} &\\sim \\mathcal N(\\mu_j, \\Sigma_j)\n",
        "\\end{cases}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq5EBriR7XXA",
        "colab_type": "text"
      },
      "source": [
        "#### E-Step:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQEHHzP27XXC",
        "colab_type": "text"
      },
      "source": [
        "$$\n",
        " w_j^{(i)} = Q_i(z^{(i)}=j) = P(z^{(i)}=j | x^{(i)}; \\phi, \\mu, \\Sigma)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkgKgj5m7XXF",
        "colab_type": "text"
      },
      "source": [
        "#### M-Step:\n",
        "Maximize the following w.r.t. $\\phi, \\mu, \\Sigma$:\n",
        "\n",
        "\\begin{align}\n",
        "&\\sum_{i=1}^m \\sum_{z^{(i)}} Q_i(z^{(i)}) \\log \\frac{p(x^{(i)}, z^{(i)}; \\phi,\\mu,\\Sigma)}{Q_i(z^{(i)})} \\\\\n",
        "&=\\sum_{i=1}^m \\sum_{j=1}^k Q_i(z^{(i)}=j) \\log\\frac{p(x^{(i)}|z^{(i)}=j; \\mu,\\Sigma)p(z^{(i)}=j; \\phi)}{Q_i(z^{(i)}=j)}\\\\\n",
        "&=\\sum_{i=1}^m \\sum_{j=1}^k w_j^{(i)} \\log\\frac{\\frac{1}{(2\\pi)^{n/2}|\\Sigma_j|^{1/2}} \\exp\\big(-\\frac12 (x^{(i)}-\\mu_j)^T\\Sigma_j^{-1}(x^{(i)}-\\mu_j)\\big)\\cdot \\phi_j}{w_j^{(i)}}\\\\\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZt1U10x7XXF",
        "colab_type": "text"
      },
      "source": [
        "Optimization gives:\n",
        "$$\n",
        "\\begin{cases}\n",
        " \\phi_j &= \\frac1m \\sum_{i=1}^m w_j^{(i)} \\\\\n",
        "\\mu_j &= \\frac{\\sum_{i=1}^m w_j^{(i)} x^{(i)}}{\\sum_{i=1}^m w_j^{(i)}}\\\\\n",
        "\\Sigma_j &= \\frac{\\sum_{i=1}^m w_j^{(i)} (x^{(i)} - \\mu_j)(x^{(i)} - \\mu_j)^T}{\\sum_{i=1}^m w_j^{(i)}}\n",
        "\\end{cases}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JL5DOJ967XXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}